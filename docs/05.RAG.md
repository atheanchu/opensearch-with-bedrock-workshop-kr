# Overview

검색 증강 생성(Retrieval Augmented Generation)은 대규모 언어 모델이 지식을 생성할 때 외부 지식원을 활용하는 기술을 말합니다.

쉽게 설명하자면, 언어 모델이 문장을 생성할 때 관련된 정보를 인터넷이나 데이터베이스에서 찾아 활용하는 것입니다. 이렇게 하면 모델이 가진 지식의 한계를 넘어 더 정확하고 상세한 내용을 생성할 수 있습니다.

예를 들어 "파리의 인구는 얼마인가?"라는 질문에 대해, 모델은 위키피디아 등에서 파리 인구 통계를 찾아 그 정보를 활용하여 대답할 수 있습니다. 단순히 학습된 지식만으로는 정확한 최신 정보를 제공하기 어렵지만, 외부 지식원을 참고하면 더 나은 결과를 낼 수 있습니다.

이 과정에서는 앞서 생성한 `movie_semantic` 인덱스를 Knowledge Base로 Amazon Bedrock의 Claude V3 Sonnet 모델을 사용하여 환각을 제거하고 정확한 답변을 생성해봅니다. 

# LangChain이란?

LangChain은 대규모 언어 모델(LLM)을 활용하여 애플리케이션을 만드는 데 도움을 주는 오픈소스 프레임워크입니다. 다양한 LLM을 지원하며, 여러 구성 요소를 체인 구조로 연결하여 복잡한 작업을 수행할 수 있습니다. 또한 대화 기록 및 외부 데이터를 메모리에 저장하고 관리하여 LLM의 응답 능력을 높입니다. 궁극적으로 LangChain을 사용하면 대화형 AI, 질의응답 시스템, 작업 자동화 등 다양한 애플리케이션을 쉽게 구축할 수 있습니다.

# Prerequisite

이번 단계를 진행하기 위해서는 [시맨틱 검색 단계](https://www.notion.so/Semantic-Search-63ccc75ad5a4497394349e59f96c0e99?pvs=21)를 필수적으로 완료하셔야 합니다.

# 사전 준비

인덱스 생성과 데이터 적재는 이미 [시맨틱 검색 단계](https://www.notion.so/Semantic-Search-63ccc75ad5a4497394349e59f96c0e99?pvs=21)에서 완료 했습니다. Amazon OpenSearch Service로의 연결은 [시맨틱 검색 단계](https://www.notion.so/Semantic-Search-63ccc75ad5a4497394349e59f96c0e99?pvs=21)와 동일하게 수행합니다. 

# 환각 일으키기 - Ask Question without RAG

RAG를 구성하지 않고 바로 Amazon Bedrock에게 다음과 같은 질문을 던져 봅니다. 

```python
query_text = "영화 건축학개론 줄거리와 평점은?"
```

질문을 던지는 코드는 다음과 같습니다. 

```python
from langchain_community.chat_models import BedrockChat
from langchain_core.messages import HumanMessage

region_name = "us-east-1"

model_kwargs = {  # anthropic
    "anthropic_version": "bedrock-2023-05-31",
    "max_tokens": 2048,
    "temperature": 0,
}

llm = BedrockChat(
    model_id="anthropic.claude-3-sonnet-20240229-v1:0",  # 파운데이션 모델 지정
    model_kwargs=model_kwargs,
    region_name=region_name,
)  # Claude 속성 구성

messages = [HumanMessage(content=query_text)]

print(textwrap.fill(llm(messages).content, 80))
```

그럼 다음과 같이 부정확한 답변이 돌아옵니다. 

> 영화 '건축학개론'은 2012년 개봉한 한국 영화입니다.   줄거리: 이 영화는 건축가를 꿈꾸는 대학생 서남준의 이야기를 그리고 있습니다.
남준은 건축학과에 입학하지만 현실과 이상 사이에서 갈등을 겪게 됩니다. 그는 자신만의 건축 철학을 찾아가는 과정에서 사랑, 우정, 꿈 등 인생의
여러 모습을 경험하게 됩니다. 건축에 대한 열정과 고민을 섬세하게 다룬 작품입니다.  평점: - 네이버 영화 평점: 7.82/10 (참여 평가
2,500여 명) - 다음 영화 관람객 평점: 3.9/5 (참여 평가 1,000여 명)  전반적으로 긍정적인 평가를 받았습니다. 건축에 대한
열정과 고민을 잘 표현했다는 호평이 많았지만, 다소 지루하다는 지적도 있었습니다. 하지만 독특한 소재와 섬세한 연출력을 인정받은 작품으로
평가되고 있습니다.
> 

# RAG 구현하기

이제 RAG을 통해 정확한 답변을 받아봅니다.

## 키워드 검색 및 Document 객체 생성

```
from langchain.schema import Document

def keyword_search(query_text):
    query = {
        "size": 10,
        "query": {
            "multi_match": {
                "query": query_text,
                "fields": ["title"],
            }
        },
    }

    res = aos_client.search(index=index_name, body=query)

    query_result = []
    for hit in res["hits"]["hits"]:
        metadata = {"score": hit["_score"], "id": hit["_id"]}

        content = {
            "title": hit["_source"]["title"],
            "genre": hit["_source"]["genre"],
            "rating": hit["_source"]["rating"],
            "text": hit["_source"]["text"],
        }

        doc = Document(page_content=json.dumps(content, ensure_ascii=False), metadata=metadata)

        query_result.append(doc)

    return query_result
```

1. 검색 결과에서 각 문서의 제목, 장르, 평점, 본문 텍스트를 추출합니다.
2. 추출된 정보와 문서 ID, 검색 점수를 포함하는 메타데이터를 사용하여 **`langchain.schema.Document`** 객체를 생성합니다.
3. 생성된 Document 객체 목록을 반환합니다.

## 컨텍스트 기반 질문 응답을 위한 프롬프트 템플릿

LangChain의 **`PromptTemplate`**을 사용하여 컨텍스트 기반 질문 응답을 위한 프롬프트 템플릿을 정의합니다.

```python
from langchain import PromptTemplate

prompt_template = """

Human: Here is the context, inside <context></context> XML tags.

<context>
{context}
</context>

Only using the contex as above, answer the following question with the rules as below:
    - Don't insert XML tag such as <context> and </context> when answering.
    - Write as much as you can
    - Be courteous and polite
    - Only answer the question if you can find the answer in the context with certainty.

Question:
{question}

If the answer is not in the context, just say "주어진 내용에서 관련 답변을 찾을 수 없습니다."

Assistant:"""

PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
```

1. 컨텍스트와 질문을 입력 변수로 받습니다.
2. 컨텍스트는 **`<context>`** 및 **`</context>`** XML 태그로 묶여 제공됩니다.
3. 답변 시 XML 태그를 포함하지 않도록 지시합니다.
4. 답변 시 가능한 한 많은 내용을 포함하도록 지시합니다.
5. 답변 시 공손하고 예의바른 태도를 유지하도록 지시합니다.
6. 컨텍스트에서 답변을 확실히 찾을 수 없는 경우, "주어진 내용에서 관련 답변을 찾을 수 없습니다."라고 답변하도록 지시합니다.

## LangChain을 사용한 질문 답변 체인 로드 및 실행

LangChain의 **`load_qa_chain`** 함수를 사용하여 질문 답변 체인을 로드하고, 주어진 문서와 질문에 대한 답변을 생성합니다.

```python
from langchain.chains.question_answering import load_qa_chain

chain = load_qa_chain(llm=llm, chain_type="stuff", prompt=PROMPT, verbose=True)

answer = chain.run(input_documents=docs, question=query_text)

print("##############################")
print("query: ", query_text)
answer_str = "answer: \n" + answer
print(textwrap.fill(answer_str, 80))
```

1. **`load_qa_chain`** 함수를 사용하여 질문 답변 체인을 로드합니다. 이 함수는 **`llm`** (언어 모델), **`chain_type`** (체인 유형), **`prompt`** (프롬프트 템플릿), **`verbose`** (출력 여부) 등의 인자를 받습니다.
2. **`chain.run`** 메서드를 호출하여 질문 답변 체인을 실행합니다. 이 메서드는 **`input_documents`** (입력 문서 목록)와 **`question`** (질문)을 인자로 받습니다.
3. 생성된 답변을 **`answer`** 변수에 저장합니다.
4. 질문과 답변을 출력합니다. 답변은 **`textwrap.fill`** 함수를 사용하여 80자 너비로 줄바꿈되어 출력됩니다.

앞서 정의된 **`PROMPT`** 프롬프트 템플릿과 **`docs`** 문서 목록, **`query_text`** 질문을 사용하여 질문 답변 체인을 실행합니다. 체인은 주어진 문서와 질문을 기반으로 답변을 생성하며, 생성된 답변은 출력됩니다. LangChain의 질문 답변 기능을 활용하여 대규모 텍스트 데이터에서 특정 질문에 대한 답변을 찾는 데 사용될 수 있습니다.

## 환각 없는 답변 확인

> 영화 "건축학개론"의 줄거리는 다음과 같습니다:  "생기 넘치지만 숫기 없던 스무 살, 건축학과 승민은 '건축학개론' 수업에서
처음 만난 음대생 서연에게 반한다. 함께 숙제를 하게 되면서 차츰 마음을 열고 친해지지만, 자신의 마음을 표현하는 데 서툰 순진한 승민은 입
밖에 낼 수 없었던 고백을 마음 속에 품은 채 작은 오해로 인해 서연과 멀어지게 된다. 15년 후 서연을 다시 만나 그녀의 집을 설계하게 되면서
두 사람 사이에 새로운 감정이 싹트게 된다."  평점은 8.67점입니다.
>