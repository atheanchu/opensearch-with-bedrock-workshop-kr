{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id 설정\n",
    "%store -r model_id\n",
    "%store -r index_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 패키지를 설치합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U --quiet opensearch-py requests\n",
    "%pip install -q boto3\n",
    "%pip install -q requests\n",
    "%pip install -q requests-aws4auth\n",
    "%pip install -q opensearch-py\n",
    "%pip install -q tqdm\n",
    "%pip install -q boto3\n",
    "%pip install -q langgraph\n",
    "%pip install -q tavily-python\n",
    "%pip install -q langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CloudFormation Stack으로부터 필요한 정보를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cfn_outputs(stackname, cfn):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)[\"Stacks\"][0][\"Outputs\"]:\n",
    "        outputs[output[\"OutputKey\"]] = output[\"OutputValue\"]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 정보를 바탕으로 인증 정보를 가져옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json\n",
    "\n",
    "# region_name = \"us-west-2\"\n",
    "session = boto3.Session()\n",
    "region_name = session.region_name\n",
    "\n",
    "cfn = boto3.client(\"cloudformation\", region_name)\n",
    "kms = boto3.client(\"secretsmanager\", region_name)\n",
    "\n",
    "stackname = \"opensearch-workshop\"\n",
    "cfn_outputs = get_cfn_outputs(stackname, cfn)\n",
    "\n",
    "aos_credentials = json.loads(\n",
    "    kms.get_secret_value(SecretId=cfn_outputs[\"OpenSearchSecret\"])[\"SecretString\"]\n",
    ")\n",
    "\n",
    "aos_host = cfn_outputs[\"OpenSearchDomainEndpoint\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenSearch Cluster에 접속하고 클라이언트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "auth = (aos_credentials[\"username\"], aos_credentials[\"password\"])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts=[{\"host\": aos_host, \"port\": 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# search_model = {\"query\": {\"match\": {\"name\": \"OpenSearch-Cohere\"}}, \"size\": 10}\n",
    "\n",
    "# response = requests.get(\n",
    "#     \"https://\" + aos_host + \"/_plugins/_ml/models/_search\", auth=auth, json=search_model\n",
    "# )\n",
    "# model_info = json.loads(response.text)\n",
    "# model_id = model_info[\"hits\"][\"hits\"][0][\"_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연결이 잘 되었는지 확인하기 위해 인덱스의 문서 수를 count합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = aos_client.count(index=index_name)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG을 위한 LLM 및 Retriever 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anthropic.claude-3-sonnet-20240229-v1:0 모델을 활용하여 생성 모델을 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model_kwargs = {  # anthropic\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 2048,\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  # 파운데이션 모델 지정\n",
    "    model_kwargs=model_kwargs,\n",
    "    region_name=region_name,\n",
    ")  # Claude 속성 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Test Bedrock\n",
    "query_text = \"어벤져스와 비슷한 영화를 추천해주세요\"\n",
    "messages = [HumanMessage(content=query_text)]\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenSearch 벡터 저장소로부터 필요한 정보를 가져오는 Retriever를 생성합니다. 이전 단계에서 활용한 Hybrid Search Retriever를 재사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever 생성\n",
    "from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "from langchain.schema import BaseRetriever\n",
    "from typing import Any, List\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "class OpenSearchHybridSearchRetriever(BaseRetriever):\n",
    "    os_client: Any\n",
    "    index_name: str\n",
    "    model_id: str\n",
    "    keyword_weight = 0.3\n",
    "    semantic_weight = 0.7\n",
    "    k = 10\n",
    "    minimum_should_match = 0\n",
    "    filter = []\n",
    "\n",
    "    def _reset_search_params(\n",
    "        self,\n",
    "    ):\n",
    "\n",
    "        self.k = 10\n",
    "        self.minimum_should_match = 0\n",
    "        self.filter = []\n",
    "        self.keyword_weight = keyword_weight\n",
    "        self.semantic_weight = semantic_weight\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query_text: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        query = {\n",
    "            \"size\": 10,\n",
    "            \"_source\": {\"exclude\": [\"text\", \"vector_field\"]},\n",
    "            \"query\": {\n",
    "                \"hybrid\": {\n",
    "                    \"queries\": [\n",
    "                        {\n",
    "                            \"multi_match\": {\n",
    "                                \"query\": query_text,\n",
    "                                \"fields\": [\"title\", \"plot\", \"genre\", \"main_act\", \"supp_act\"],\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"neural\": {\n",
    "                                \"vector_field\": {\n",
    "                                    \"query_text\": query_text,\n",
    "                                    \"model_id\": model_id,\n",
    "                                    \"k\": 30,\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"search_pipeline\": {\n",
    "                \"description\": \"Post processor for hybrid search\",\n",
    "                \"phase_results_processors\": [\n",
    "                    {\n",
    "                        \"normalization-processor\": {\n",
    "                            \"normalization\": {\"technique\": \"min_max\"},\n",
    "                            \"combination\": {\n",
    "                                \"technique\": \"arithmetic_mean\",\n",
    "                                \"parameters\": {\n",
    "                                    \"weights\": [self.keyword_weight, self.semantic_weight]\n",
    "                                },\n",
    "                            },\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "        res = self.os_client.search(index=index_name, body=query)\n",
    "\n",
    "        query_result = []\n",
    "\n",
    "        for hit in res[\"hits\"][\"hits\"]:\n",
    "            metadata = {\"score\": hit[\"_score\"], \"id\": hit[\"_id\"]}\n",
    "\n",
    "            content = {\n",
    "                \"제목\": hit[\"_source\"][\"title\"],\n",
    "                \"장르\": hit[\"_source\"][\"genre\"],\n",
    "                \"평점\": hit[\"_source\"][\"rating\"],\n",
    "                \"줄거리\": hit[\"_source\"][\"plot\"],\n",
    "                \"주연\": hit[\"_source\"][\"main_act\"],\n",
    "                \"조연\": hit[\"_source\"][\"supp_act\"],\n",
    "            }\n",
    "\n",
    "            doc = Document(page_content=json.dumps(content, ensure_ascii=False), metadata=metadata)\n",
    "            query_result.append(doc)\n",
    "        return query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vectorDB\n",
    "retriever = OpenSearchHybridSearchRetriever(\n",
    "    os_client=aos_client, index_name=index_name, model_id=model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(query_text)\n",
    "\n",
    "# docs\n",
    "\n",
    "for doc in docs:\n",
    "    d = json.loads(doc.page_content)\n",
    "    print(json.dumps(d, indent=2, ensure_ascii=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self RAG\n",
    "\n",
    "Self-RAG은 자기 평가을 통해 정보를 검색하고 생성하는 새로운 Advanced RAG 기법입니다.\n",
    "\n",
    "1. **자기 평가 메커니즘**: Self-RAG는 '평가 토큰'이라는 특별한 토큰을 사용하여 모델이 생성한 텍스트의 품질을 자체적으로 평가합니다.\n",
    "\n",
    "2. **적응적 검색**: 모델은 필요에 따라 문서를 검색하고, 생성된 내용과 검색된 문서를 평가합니다.\n",
    "\n",
    "3. **사용자 맞춤화**: 평가 토큰 예측을 통해 검색 빈도를 조정하고 사용자 선호에 맞게 모델 동작을 맞춤화할 수 있습니다.\n",
    "\n",
    "4. **품질 및 사실성 향상**: Self-RAG는 대규모 언어 모델의 출력 품질과 사실성을 향상시키는 것을 목표로 합니다.\n",
    "\n",
    "Self-RAG는 다양한 작업에서 뛰어난 성능을 보여줍니다:\n",
    "\n",
    "- 개방형 질문-응답, 추론, 사실 검증 작업 등에서 최신 언어 모델과 검색 기반 모델을 능가하는 성능을 보입니다.\n",
    "- 특히 장문 생성에서 사실 정확성과 인용 정확도를 크게 향상시킵니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검색된 문서의 관련성을 평가하는 Retrieval Grader를 구현합니다. LangChain Hub에서 가져온 프롬프트와 구조화된 출력을 지원하는 LLM을 결합하여, 주어진 질문에 대해 검색된 문서가 관련 있는지 여부를 'yes' 또는 'no'로 평가하는 grader를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "\n",
    "# https://smith.langchain.com/hub/efriis/self-rag-retrieval-grader\n",
    "grade_prompt = hub.pull(\"efriis/self-rag-retrieval-grader\")\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the retrieval grader\n",
    "docs = retriever.invoke(query_text)\n",
    "doc_txt = docs[0].page_content\n",
    "print(doc_txt)\n",
    "print(retrieval_grader.invoke({\"question\": query_text, \"document\": doc_txt}))\n",
    "print(retrieval_grader.invoke({\"question\": \"슈퍼히어로가 나오는 영화\", \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": query_text})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성된 답변에서 환각(hallucination) 여부를 평가하는 Hallucination Grader를 구현합니다. LangChain Hub에서 가져온 프롬프트와 구조화된 출력을 지원하는 LLM을 결합하여, 주어진 문서들을 기반으로 생성된 답변이 사실에 근거하는지 여부를 'yes' 또는 'no'로 평가하는 grader를 생성하고 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# https://smith.langchain.com/hub/efriis/self-rag-hallucination-grader\n",
    "hallucination_prompt = hub.pull(\"efriis/self-rag-hallucination-grader\")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "print(generation)\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성된 답변이 질문을 적절히 다루는지 평가하는 Answer Grader를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer Grader\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n",
    "\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt\n",
    "answer_prompt = hub.pull(\"efriis/self-rag-answer-grader\")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "print(query_text)\n",
    "print(generation)\n",
    "answer_grader.invoke({\"question\": query_text, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph를 활용하여 Self-RAG 구현하기\n",
    "\n",
    "이번 과정에서는 Self-RAG 워크플로우를 구현하기 위해 LangGraph를 사용합니다. LangGraph란 LangChain의 확장 라이브러리로 복잡한 Multi-Agent 시스템을 구축하기 위한 도구입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GraphState` 클래스는 그래프의 상태를 나타내는 데 사용됩니다. 예를 들어, LangGraph나 유사한 프레임워크에서 그래프 기반 워크플로우의 각 노드 간에 전달되는 상태 정보를 타입 안전하게 정의하고 관리하는 데 활용될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-RAG를 구현할 Graph에서 각 노드가 사용할 모듈을 정의합니다. \n",
    "\n",
    "• retrieve 함수:\n",
    "  - 주어진 질문에 대해 관련 문서를 검색합니다.\n",
    "  - 검색된 문서를 상태 딕셔너리에 추가합니다.\n",
    "\n",
    "• generate 함수:\n",
    "  - 검색된 문서와 질문을 바탕으로 답변을 생성합니다.\n",
    "  - RAG(Retrieval-Augmented Generation) 체인을 사용하여 답변을 생성합니다.\n",
    "\n",
    "• grade_documents 함수:\n",
    "  - 검색된 문서가 질문과 관련이 있는지 평가합니다.\n",
    "  - 관련성이 높은 문서만 필터링하여 상태를 업데이트합니다.\n",
    "\n",
    "• transform_query 함수:\n",
    "  - 주어진 질문을 더 나은 형태로 변환합니다.\n",
    "  - 변환된 질문으로 상태를 업데이트합니다.\n",
    "\n",
    "각 함수는 상태 딕셔너리를 입력으로 받아 처리한 후, 업데이트된 상태를 반환합니다. 이 함수들은 질문 응답 시스템의 파이프라인을 구성하며, 문서 검색, 답변 생성, 문서 관련성 평가, 질문 개선 등의 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nodes\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "엣지(결정 로직)를 정의하고 있습니다.\n",
    "\n",
    "• decide_to_generate 함수:\n",
    "  - 답변을 생성할지 또는 질문을 재생성할지 결정합니다.\n",
    "  - 필터링된 문서가 없으면 질문을 변환하고, 있으면 답변을 생성합니다.\n",
    "\n",
    "• grade_generation_v_documents_and_question 함수:\n",
    "  - 생성된 답변이 문서에 근거하고 있는지, 그리고 질문에 적절히 답변하는지 평가합니다.\n",
    "  - 평가 결과에 따라 다음 단계를 결정합니다:\n",
    "    1. 답변이 문서에 근거하고 질문에 적절하면 \"useful\" 반환\n",
    "    2. 답변이 문서에 근거하지만 질문에 부적절하면 \"not useful\" 반환\n",
    "    3. 답변이 문서에 근거하지 않으면 \"not supported\" 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 노드 및 엣지 모듈을 사용하여 LangGraph 워크플로우를 정의합니다.\n",
    "\n",
    "• StateGraph 객체 생성:\n",
    "  - GraphState를 사용하여 워크플로우의 상태를 관리합니다.\n",
    "\n",
    "• 노드 추가:\n",
    "  - retrieve: 문서 검색\n",
    "  - grade_documents: 문서 평가\n",
    "  - generate: 답변 생성\n",
    "  - transform_query: 질문 변환\n",
    "\n",
    "• 엣지 추가:\n",
    "  - START에서 retrieve로 시작합니다.\n",
    "  - retrieve에서 grade_documents로 이동합니다.\n",
    "  - grade_documents에서 조건부로 transform_query 또는 generate로 이동합니다.\n",
    "  - transform_query에서 다시 retrieve로 돌아갑니다.\n",
    "  - generate에서 조건부로 다음 단계를 결정합니다:\n",
    "    1. \"not supported\": 다시 generate로\n",
    "    2. \"useful\": 워크플로우 종료 (END)\n",
    "    3. \"not useful\": transform_query로\n",
    "\n",
    "• 워크플로우 컴파일:\n",
    "  - 정의된 그래프를 실행 가능한 애플리케이션으로 컴파일합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-RAG을 테스트해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": query_text}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"외계인과 싸우는 영화 추천해줘\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRAG(Corrective RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRAG이란 자체 평가를 통해 실시간으로 질문을 재작성하고 답변을 보정하는 Advanced RAG 기법입니다. CRAG은 다음과 같은 장점이 있습니다.\n",
    "\n",
    "- CRAG는 기존 RAG 시스템에 비해 더 정확하고 관련성 높은 정보를 제공합니다.\n",
    "- 플러그 앤 플레이 방식으로 다양한 RAG 시스템에 쉽게 적용할 수 있습니다.\n",
    "- Self-RAG에 비해 더 경량화되어 있어 효율적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 과정을 수행하기 위해서는 Tavily API Key를 발급받아야 합니다. https://app.tavily.com/home에 Sign Up하고 API를 복사하여 아래와 같이 환경변수에 등록합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"your tavily API key\"\n",
    "\n",
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 검색 시스템의 관련성 평가 도구를 생성합니다.\n",
    "\n",
    "1. `GradeDocuments` 클래스로 이진 점수(yes/no) 모델을 정의합니다.\n",
    "\n",
    "2. LLM을 사용해 구조화된 출력을 생성하는 그레이더를 설정합니다.\n",
    "\n",
    "3. 관련성 평가를 위한 프롬프트를 ChatPromptTemplate로 정의합니다.\n",
    "\n",
    "4. 프롬프트와 LLM 그레이더를 결합해 `retrieval_grader`를 생성합니다.\n",
    "\n",
    "5. 주어진 한국어 질문에 대해 문서를 검색하고, 두 번째 문서의 관련성을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"우주에서 외계인과 싸우는 이야기\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "print(docs[1].page_content)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영화 관련 질문을 개선하는 질문 재작성기를 생성합니다.\n",
    "\n",
    "1. 시스템 프롬프트를 정의하여 입력 질문의 의미를 파악하고 영화 검색에 최적화된 버전으로 재작성하도록 지시합니다.\n",
    "\n",
    "2. `ChatPromptTemplate`을 사용해 시스템 메시지와 사용자 메시지를 포함한 프롬프트를 구성합니다.\n",
    "\n",
    "3. `question_rewriter`를 생성합니다:\n",
    "   - 재작성 프롬프트\n",
    "   - LLM(대규모 언어 모델)\n",
    "   - `StrOutputParser()`를 연결하여 문자열 출력을 생성\n",
    "\n",
    "4. `invoke` 메서드로 주어진 질문을 재작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question Re-writer\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer about movies that converts an input question to a better version that is optimized \\n \n",
    "     for movie search. Look at the input and try to reason about the underlying semantic intent / meaning. \n",
    "     The response should be one simple rewritten question that captures the essence of the original question.\n",
    "     \"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-RAG와 마찬가지로 GraphState를 정의합니다. 여기서는 웹 검색을 위한 state인 `web_search`가 추가되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-RAG와 동일하게 Graph의 각 노드가 사용할 모듈을 정의합니다. 여기서는 생성을 보강하기 위한 웹 검색의 도구로 web_search 모듈이 추가됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question with one simple sentence\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    print(\"question:\", question)\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph 워크플로우를 컴파일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "workflow.add_node(\"web_search_node\", web_search)  # web search\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRAG을 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": query_text}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터 저장소에 저장된 정보로 대답할 수 없는 정보의 경우 웹 검색으로 답변을 보강하여 생성하는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"2024년 개봉한 슈퍼 히어로 영화 추천해줘\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensearch-workhop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
