{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 대화형 검색 - Conversational Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "대화형 검색은 사용자가 자연스럽게 질문하면 인공지능 시스템이 이해하고 관련 정보를 제공하는 방식을 말합니다. 단순히 키워드를 검색하는 것이 아니라, 실제 대화를 통해 정보를 얻습니다. 이는 사용자가 보다 자연스럽게 원하는 정보를 찾을 수 있게 해주며, 키워드를 입력하는 대신 대화를 통해 원하는 내용을 명확히 전달할 수 있습니다. 더불어, 지속적인 대화를 통해 추가적인 정보를 요청하거나 세부 사항을 파악하는 것도 가능합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전 준비\n",
    "\n",
    "이번 단계를 진행하기 위해서는 [시맨틱 검색 단계](./02.semantic_search.ipynb)를 필수적으로 완료하셔야 합니다. Amazon OpenSearch Service로의 연결은 [시맨틱 검색 단계](./02.semantic_search.ipynb)와 동일하게 수행합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패키지를 설치하고 import 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q boto3\n",
    "!pip install -q requests\n",
    "!pip install -q requests-aws4auth\n",
    "!pip install -q opensearch-py\n",
    "!pip install -q tqdm\n",
    "!pip install -q boto3\n",
    "!pip install -q langchain\n",
    "!pip install -q langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "import boto3\n",
    "import json\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CloudFormation에서 필요한 정보를 가져와 OpenSearch 도메인에 연결합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cfn_outputs(stackname, cfn):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)[\"Stacks\"][0][\"Outputs\"]:\n",
    "        outputs[output[\"OutputKey\"]] = output[\"OutputValue\"]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json\n",
    "\n",
    "# region_name = \"us-west-2\"\n",
    "session = boto3.Session()\n",
    "region_name = session.region_name\n",
    "\n",
    "cfn = boto3.client(\"cloudformation\", region_name)\n",
    "kms = boto3.client(\"secretsmanager\", region_name)\n",
    "\n",
    "stackname = \"opensearch-workshop\"\n",
    "cfn_outputs = get_cfn_outputs(stackname, cfn)\n",
    "\n",
    "aos_credentials = json.loads(\n",
    "    kms.get_secret_value(SecretId=cfn_outputs[\"OpenSearchSecret\"])[\"SecretString\"]\n",
    ")\n",
    "\n",
    "aos_host = cfn_outputs[\"OpenSearchDomainEndpoint\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "auth = (aos_credentials[\"username\"], aos_credentials[\"password\"])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts=[{\"host\": aos_host, \"port\": 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연결이 잘 되었는지 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "search_model = {\"query\": {\"match\": {\"name\": \"OpenSearch-Cohere\"}}, \"size\": 10}\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://\" + aos_host + \"/_plugins/_ml/models/_search\", auth=auth, json=search_model\n",
    ")\n",
    "model_info = json.loads(response.text)\n",
    "model_id = model_info[\"hits\"][\"hits\"][0][\"_id\"]\n",
    "\n",
    "index_name = \"movie_semantic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = aos_client.count(index=index_name)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 대화형 검색 구현하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain을 사용한 대화형 AI 에이전트 구성\n",
    "\n",
    "LangChain 라이브러리를 사용하여 Anthropic의 Claude 언어 모델을 기반으로 한 대화형 AI 에이전트를 구성합니다. 여기서는 **`ConversationBufferWindowMemory`**를 사용하여 최근 10개의 메시지를 기억하는 대화 기억 장치를 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model_kwargs = {  # anthropic\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 2048,\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  # 파운데이션 모델 지정\n",
    "    model_kwargs=model_kwargs,\n",
    "    region_name=region_name,\n",
    ")  # Claude 속성 구성\n",
    "\n",
    "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", k=10, return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSearch의 하이브리드 Retriever 객체 생성\n",
    "\n",
    "OpenSearch 엔진을 사용하여 하이브리드 검색을 수행하는 OpenSearchHybridSearchRetriever를 정의합니다. 이 검색기는 LangChain의 BaseRetriever 클래스를 상속받고 있으며, 주요 기능은 다음과 같습니다. OpenSearchHybridSearchRetriever는 추후 대화형 검색 체인에서 retriever 역할을 담당하게 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "from langchain.schema import BaseRetriever\n",
    "from typing import Any, List\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "class OpenSearchHybridSearchRetriever(BaseRetriever):\n",
    "    os_client: Any\n",
    "    index_name: str\n",
    "    model_id: str\n",
    "    keyword_weight = 0.3\n",
    "    semantic_weight = 0.7\n",
    "    k = 10\n",
    "    minimum_should_match = 0\n",
    "    filter = []\n",
    "\n",
    "    def _reset_search_params(\n",
    "        self,\n",
    "    ):\n",
    "\n",
    "        self.k = 10\n",
    "        self.minimum_should_match = 0\n",
    "        self.filter = []\n",
    "        self.keyword_weight = keyword_weight\n",
    "        self.semantic_weight = semantic_weight\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query_text: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        query = {\n",
    "            \"size\": 10,\n",
    "            \"_source\": {\"exclude\": [\"text\", \"vector_field\"]},\n",
    "            \"query\": {\n",
    "                \"hybrid\": {\n",
    "                    \"queries\": [\n",
    "                        {\n",
    "                            \"multi_match\": {\n",
    "                                \"query\": query_text,\n",
    "                                \"fields\": [\"title\", \"plot\", \"genre\", \"main_act\", \"supp_act\"],\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"neural\": {\n",
    "                                \"vector_field\": {\n",
    "                                    \"query_text\": query_text,\n",
    "                                    \"model_id\": model_id,\n",
    "                                    \"k\": 30,\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"search_pipeline\": {\n",
    "                \"description\": \"Post processor for hybrid search\",\n",
    "                \"phase_results_processors\": [\n",
    "                    {\n",
    "                        \"normalization-processor\": {\n",
    "                            \"normalization\": {\"technique\": \"min_max\"},\n",
    "                            \"combination\": {\n",
    "                                \"technique\": \"arithmetic_mean\",\n",
    "                                \"parameters\": {\n",
    "                                    \"weights\": [self.keyword_weight, self.semantic_weight]\n",
    "                                },\n",
    "                            },\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "        res = self.os_client.search(index=index_name, body=query)\n",
    "\n",
    "        query_result = []\n",
    "\n",
    "        for hit in res[\"hits\"][\"hits\"]:\n",
    "            metadata = {\"score\": hit[\"_score\"], \"id\": hit[\"_id\"]}\n",
    "\n",
    "            content = {\n",
    "                \"제목\": hit[\"_source\"][\"title\"],\n",
    "                \"장르\": hit[\"_source\"][\"genre\"],\n",
    "                \"평점\": hit[\"_source\"][\"rating\"],\n",
    "                \"줄거리\": hit[\"_source\"][\"plot\"],\n",
    "                \"주연\": hit[\"_source\"][\"main_act\"],\n",
    "                \"조연\": hit[\"_source\"][\"supp_act\"],\n",
    "            }\n",
    "\n",
    "            doc = Document(page_content=json.dumps(content, ensure_ascii=False), metadata=metadata)\n",
    "            query_result.append(doc)\n",
    "        return query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프롬프트 템플릿 정의\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 추천 기본 프롬프트 정의\n",
    "\n",
    "이 프롬프트는 목록에서 특정 질문에 대한 답변을 생성하는 프롬프트 템플릿을 정의합니다. 프롬프트 템플릿은 주어진 컨텍스트(영화 목록)와 질문을 입력받아 답변을 생성하는 형식으로 구성되어 있습니다. 답변 생성 시에는 정해진 규칙(예의바른 태도, 목록 형식, 간단한 설명 등)을 따르도록 되어 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "\n",
    "Human: Here is the list of recommended movies, inside <movies></movies> XML tags.\n",
    "\n",
    "<movies>\n",
    "{context}\n",
    "</movies>\n",
    "\n",
    "Only using the contex as above, answer the following question with the rules as below:\n",
    "    - Don't insert XML tag such as <context> and </context> when answering.\n",
    "    - Write as much as you can\n",
    "    - Be courteous and polite\n",
    "    - Only answer the question if you can find the answer in the context with certainty.\n",
    "    - Answered in list format\n",
    "    - Always put a short and concise explanation on why you are recommending this movies.\n",
    "\n",
    "You are a best movie reviewer in Korea. Please recommend a movies from the list above.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "If the answer is not in the context, just say \"추천해드릴만한 영화가 없습니다.\"\n",
    "\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condense 템플릿 정의\n",
    "\n",
    "이 프롬프트 템플릿은 대화 내역과 후속 질문을 입력받아 독립적인 질문을 생성하는 데 사용됩니다. 템플릿의 구조는 다음과 같습니다:\n",
    "\n",
    "1. **지시사항(instructions)**: 이 섹션에서는 프롬프트의 목적과 입력 형식에 대한 설명을 제공합니다.\n",
    "2. **대화 내역(chat_history)**: 여기에는 이전 대화 내용이 포함됩니다. **`{chat_history}`** 부분이 실제 대화 내역으로 대체됩니다.\n",
    "3. **후속 질문(follow-up-question)**: 이 섹션에는 대화 내역에 대한 후속 질문이 포함됩니다. **`{question}`** 부분이 실제 후속 질문으로 대체됩니다.\n",
    "4. **독립 질문(standalone question)**: 이 부분에서 모델은 대화 내역과 후속 질문을 요약한 독립적인 질문을 생성해야 합니다.\n",
    "\n",
    "이 프롬프트 템플릿을 사용하면 LangChain의 **`CONDENSE_QUESTION_PROMPT`**에 대화 내역과 후속 질문을 입력할 수 있습니다. 그러면 모델이 이전 대화 내용과 후속 질문을 종합하여 독립적인 질문을 생성합니다. 이렇게 생성된 질문은 대화 맥락을 유지하면서도 독립적으로 이해할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_template = \"\"\"\n",
    "Generate one standalone question based on the instructions.\n",
    "\n",
    "<instrunctions>\n",
    "- You will be given the following conversation between <chat-history> and </chat-history>\n",
    "- You will be given the following follow up question between <follow-up-question> and </follow-up-question>\n",
    "- Standalone question should have summary of the previous questions and answers.\n",
    "</instructions>\n",
    "\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-question>\n",
    "{question}\n",
    "</follow-up-question>\n",
    "\n",
    "standalone question:\n",
    "\"\"\"\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(condense_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationalRetrievalChain을 통해 대화형 검색을 위한 Chain을 구성합니다. 문서 검색을 위해 앞서 정의한 OpenSearchHybridSearchRetriever를 retriever로 제공하고, 검색 히스토리가 저장되는 memory 객체를 전달합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory.clear()\n",
    "\n",
    "hybrid_retriever = OpenSearchHybridSearchRetriever(\n",
    "    os_client=aos_client, index_name=index_name, model_id=model_id\n",
    ")\n",
    "\n",
    "conversation_with_retrieval = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=hybrid_retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    # verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 대화형 검색 테스트\n",
    "\n",
    "이제 실제로 대화형 검색을 시도해봅시다. 먼저 첫 번째 질문으로 영화를 추천받습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_question = \"초능력을 가진 영웅들이 지구를 지킨다\"\n",
    "chat_response = conversation_with_retrieval.invoke({\"question\": first_question})\n",
    "\n",
    "print(textwrap.fill(chat_response[\"answer\"], 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_question = \"그 영화 평점은?\"\n",
    "chat_response = conversation_with_retrieval.invoke({\"question\": second_question})\n",
    "\n",
    "print(textwrap.fill(chat_response[\"answer\"], 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_question = \"비슷한 장르의 다른 영화는?\"\n",
    "chat_response = conversation_with_retrieval.invoke({third_question})\n",
    "\n",
    "print(textwrap.fill(chat_response[\"answer\"], 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
