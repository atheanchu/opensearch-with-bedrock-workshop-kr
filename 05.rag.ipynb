{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "검색 증강 생성(Retrieval Augmented Generation)은 대규모 언어 모델이 지식을 생성할 때 외부 지식원을 활용하는 기술을 말합니다.\n",
    "\n",
    "쉽게 설명하자면, 언어 모델이 문장을 생성할 때 관련된 정보를 인터넷이나 데이터베이스에서 찾아 활용하는 것입니다. 이렇게 하면 모델이 가진 지식의 한계를 넘어 더 정확하고 상세한 내용을 생성할 수 있습니다.\n",
    "\n",
    "예를 들어 \"파리의 인구는 얼마인가?\"라는 질문에 대해, 모델은 위키피디아 등에서 파리 인구 통계를 찾아 그 정보를 활용하여 대답할 수 있습니다. 단순히 학습된 지식만으로는 정확한 최신 정보를 제공하기 어렵지만, 외부 지식원을 참고하면 더 나은 결과를 낼 수 있습니다.\n",
    "\n",
    "이 과정에서는 앞서 생성한 `movie_semantic` 인덱스를 Knowledge Base로 Amazon Bedrock의 Claude V3 Sonnet 모델을 사용하여 환각을 제거하고 정확한 답변을 생성해봅니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전준비\n",
    "\n",
    "이번 단계를 진행하기 위해서는 [시맨틱 검색 단계](./02.semantic_search.ipynb)를 필수적으로 완료하셔야 합니다. Amazon OpenSearch Service로의 연결은 [시맨틱 검색 단계](./02.semantic_search.ipynb)와 동일하게 수행합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r model_id\n",
    "%store -r index_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패키지를 설치합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q boto3\n",
    "!pip install -U -q requests\n",
    "!pip install -U -q requests-aws4auth\n",
    "!pip install -U -q opensearch-py\n",
    "!pip install -U -q tqdm\n",
    "!pip install -U -q boto3\n",
    "!pip install -U -q langchain\n",
    "!pip install -U -q langchain_community\n",
    "!pip install -U -q langchain-aws\n",
    "!pip install -U -q python-dotenv\n",
    "!pip install --upgrade --quiet langchain_experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환각 발생시키기\n",
    "\n",
    "RAG를 구성하지 않고 바로 Amazon Bedrock Claude Sonnet V3에게 다음과 같은 질문을 던져 봅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "import boto3\n",
    "import json\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"건축학개론 내용과 평점은?\"\n",
    "print(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "session = boto3.Session()\n",
    "region_name = session.region_name\n",
    "\n",
    "model_kwargs = {  # anthropic\n",
    "    \"max_tokens\": 2048,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  # 파운데이션 모델 지정\n",
    "    # client=bedrock_client,\n",
    "    region_name=region_name,\n",
    "    model_kwargs=model_kwargs,\n",
    ")  # Claude 속성 구성\n",
    "\n",
    "\n",
    "messages = [HumanMessage(content=query_text)]\n",
    "\n",
    "print(textwrap.fill(llm.invoke(messages).content, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Bedrock Claude Sonnet V3은 한국 영화 건축학 개론에 대한 답을 모르기 때문에 엉뚱한 답변을 하게 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG을 통해 환각 없애기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 OpenSearch에 저장된 정보를 컨텍스트로 전달하여 환각을 없애보겠습니다. 여기서는 이전 단계에서 사용한 [하이브리드 검색 기법](./03.hybrid_search.ipynb)을 활용해보도록 하겠습니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSearch 클라이언트 연결\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 과정에서 수행했던 것과 동일한 방법으로 OpenSearch 도메인에 연결합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cfn_outputs(stackname, cfn):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)[\"Stacks\"][0][\"Outputs\"]:\n",
    "        outputs[output[\"OutputKey\"]] = output[\"OutputValue\"]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 CloudFormation 스택으로보터 호스트와 인증 정보를 가져옵니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json\n",
    "\n",
    "cfn = boto3.client(\"cloudformation\", region_name)\n",
    "kms = boto3.client(\"secretsmanager\", region_name)\n",
    "\n",
    "stackname = \"opensearch-workshop\"\n",
    "cfn_outputs = get_cfn_outputs(stackname, cfn)\n",
    "\n",
    "aos_credentials = json.loads(\n",
    "    kms.get_secret_value(SecretId=cfn_outputs[\"OpenSearchSecret\"])[\"SecretString\"]\n",
    ")\n",
    "\n",
    "aos_host = cfn_outputs[\"OpenSearchDomainEndpoint\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가져온 정보를 바탕으로 OpenSearch 도메인에 연결합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "auth = (aos_credentials[\"username\"], aos_credentials[\"password\"])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts=[{\"host\": aos_host, \"port\": 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인덱스명과 모델 아이디를 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# search_model = {\"query\": {\"match\": {\"name\": \"OpenSearch-Cohere\"}}, \"size\": 10}\n",
    "\n",
    "# response = requests.get(\n",
    "#     \"https://\" + aos_host + \"/_plugins/_ml/models/_search\", auth=auth, json=search_model\n",
    "# )\n",
    "# model_info = json.loads(response.text)\n",
    "# model_id = model_info[\"hits\"][\"hits\"][0][\"_id\"]\n",
    "\n",
    "# index_name = \"movie_semantic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이브리드 검색 함수 구현\n",
    "\n",
    "이전 [하이브리드 검색 단계](https://www.notion.so/Hybrid-Search-9b923c1a7b2e4d1697768c3385ea47d0?pvs=21)에서 사용한 `hybrid_search` 함수를 다음과 같이 변경합니다. 가장 큰 변경 부분은 검색된 결과를 LangChain의 Document 객체로 변환해서 반환한다는 점입니다. 이렇게 생성된 **`Document`** 객체들을 기반으로 LangChain은 다양한 작업을 수행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def hybrid_search(\n",
    "    query_text,\n",
    "    keyword_weight=0.3,\n",
    "    semantic_weight=0.7,\n",
    "):\n",
    "    print(query_text)\n",
    "    query = {\n",
    "        \"size\": 10,\n",
    "        \"_source\": {\"exclude\": [\"text\", \"vector_field\"]},\n",
    "        \"query\": {\n",
    "            \"hybrid\": {\n",
    "                \"queries\": [\n",
    "                    {\n",
    "                        \"multi_match\": {\n",
    "                            \"query\": query_text,\n",
    "                            \"fields\": [\"title\", \"plot\", \"genre\", \"main_act\", \"supp_act\"],\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"neural\": {\n",
    "                            \"vector_field\": {\n",
    "                                \"query_text\": query_text,\n",
    "                                \"model_id\": model_id,\n",
    "                                \"k\": 30,\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"search_pipeline\": {\n",
    "            \"description\": \"Post processor for hybrid search\",\n",
    "            \"phase_results_processors\": [\n",
    "                {\n",
    "                    \"normalization-processor\": {\n",
    "                        \"normalization\": {\"technique\": \"min_max\"},\n",
    "                        \"combination\": {\n",
    "                            \"technique\": \"arithmetic_mean\",\n",
    "                            \"parameters\": {\"weights\": [keyword_weight, semantic_weight]},\n",
    "                        },\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, body=query)\n",
    "\n",
    "    query_result = []\n",
    "    docs = []\n",
    "    for hit in res[\"hits\"][\"hits\"]:\n",
    "        row = [\n",
    "            hit[\"_score\"],\n",
    "            hit[\"_source\"][\"title\"],\n",
    "            hit[\"_source\"][\"plot\"],\n",
    "            hit[\"_source\"][\"genre\"],\n",
    "            hit[\"_source\"][\"rating\"],\n",
    "            hit[\"_source\"][\"main_act\"],\n",
    "        ]\n",
    "        query_result.append(row)\n",
    "\n",
    "        # LangChain에 Context로 제공하기 위한 Document 객체를 준비합니다.\n",
    "        metadata = {\"score\": hit[\"_score\"], \"id\": hit[\"_id\"]}\n",
    "\n",
    "        content = {\n",
    "            \"제목\": hit[\"_source\"][\"title\"],\n",
    "            \"장르\": hit[\"_source\"][\"genre\"],\n",
    "            \"평점\": hit[\"_source\"][\"rating\"],\n",
    "            \"줄거리\": hit[\"_source\"][\"plot\"],\n",
    "            \"주연\": hit[\"_source\"][\"main_act\"],\n",
    "            \"조연\": hit[\"_source\"][\"supp_act\"],\n",
    "        }\n",
    "\n",
    "        doc = Document(page_content=json.dumps(content, ensure_ascii=False), metadata=metadata)\n",
    "\n",
    "        docs.append(doc)\n",
    "\n",
    "    query_result_df = pd.DataFrame(\n",
    "        data=query_result, columns=[\"_score\", \"title\", \"plot\", \"genre\", \"rating\", \"main_act\"]\n",
    "    )\n",
    "    display(query_result_df)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검색 함수가 잘 동작하는지 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = hybrid_search(query_text)\n",
    "\n",
    "print(\"검색된 문서 총 \" + str(len(docs)) + \"개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG을 통한 정확한 답변 받기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컨텍스트 기반 질문 응답을 위한 프롬프트 템플릿 생성\n",
    "\n",
    "LangChain의 **`PromptTemplate`**을 사용하여 컨텍스트 기반 질문 응답을 위한 프롬프트 템플릿을 정의합니다.\n",
    "\n",
    "1. 컨텍스트와 질문을 입력 변수로 받습니다.\n",
    "2. 컨텍스트는 **`<context>`** 및 **`</context>`** XML 태그로 묶여 제공됩니다.\n",
    "3. 답변 시 XML 태그를 포함하지 않도록 지시합니다.\n",
    "4. 답변 시 가능한 한 많은 내용을 포함하도록 지시합니다.\n",
    "5. 답변 시 공손하고 예의바른 태도를 유지하도록 지시합니다.\n",
    "6. 컨텍스트에서 답변을 확실히 찾을 수 없는 경우, \"주어진 내용에서 관련 답변을 찾을 수 없습니다.\"라고 답변하도록 지시합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "\n",
    "Human: Here is the context, inside <context></context> XML tags.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Only using the contex as above, answer the following question with the rules as below:\n",
    "    - Don't insert XML tag such as <context> and </context> when answering.\n",
    "    - Write as much as you can\n",
    "    - Be courteous and polite\n",
    "    - Only answer the question if you can find the answer in the context with certainty.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "If the answer is not in the context, just say \"주어진 내용에서 관련 답변을 찾을 수 없습니다.\"\n",
    "\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain의 **`load_qa_chain`** 함수를 사용하여 질문 답변 체인을 로드하고, 주어진 문서와 질문에 대한 답변을 생성합니다. 하이브리드 검색으로 찾은 문서 `docs`를 `chain`의 `input_documents`로 제공합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 2048,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "        \"temperature\": 0,\n",
    "    },\n",
    ")\n",
    "\n",
    "chain = load_qa_chain(llm=llm, chain_type=\"stuff\", prompt=PROMPT)\n",
    "\n",
    "answer = chain.run(input_documents=docs, question=query_text)\n",
    "\n",
    "print(\"query: \", query_text)\n",
    "answer_str = \"answer: \\n\" + answer\n",
    "print(textwrap.fill(answer_str, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 정확한 답변이 제공되는 것을 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store model_id\n",
    "%store index_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
